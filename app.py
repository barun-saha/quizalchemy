"""
QuizAlchemy: Transmute text into knowledge

A quiz generation application using Google ADK and Streamlit.
"""
import json
import logging
import mimetypes
import random
import re
import sqlite3
import uuid
import warnings
import asyncio
from typing import Optional

import litellm
import streamlit as st
from markitdown import MarkItDown
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from google.adk.agents import Agent
from google.adk.models.lite_llm import LiteLlm
from google.adk.sessions import InMemorySessionService
from google.adk.runners import Runner
from google.genai import types


FILE_URL = 'https://web.stanford.edu/class/cs102/lectureslides/ClassificationSlides.pdf'
APP_NAME = 'quizalchemy'
MODEL_GEMINI = 'gemini/gemini-2.0-flash-lite'
DIFFICULTY_LEVELS = ['easy', 'medium', 'hard']

QUESTION_BANK_PROMPT = '''
You are an expert tutor. Given the text below, create a question bank having 10--30 questions
and their answers. The questions provide multiple choices where only one choice is correct.
The `answer_key` field will contain the list index (0-based) of the correct answer in the `choices`
list. In addition, create questions with three levels of difficulties: easy, medium, and hard.
About 40% of the questions should be easy, 30% medium, and 30% hard.
Create the question-answer pairs in a way that promotes critical thinking among the students.
IMPORTANT: The questions and answers must be based solely and only on the provided text
and must be factually correct.
Also, create the questions and answers in the same language as the input text.

An illustrative example:
{{
    "questions": [
        {{
            "question": "What is the capital of India?",
            "choices": ["New Delhi", "Mumbai", "Kolkata", "Chennai"],
            "answer_key": 0,
            "difficulty": "easy"
        }}
    ]
}}

# Text
{text}
'''


load_dotenv()
logging.basicConfig(level=logging.ERROR)
warnings.filterwarnings('ignore')

llm = LiteLlm(model=MODEL_GEMINI)

st.title('QuizAlchemy')
st.markdown('## *Transmute text into knowledge* ðŸ’Ž')


class QuestionAnswer(BaseModel):
    """
    Represents a question and its multiple choice answers.
    """
    id: Optional[int] = Field(
        None,
        description='The question number, auto-generated by DB. Skip it.'
    )
    question: str = Field(description='The question text.')
    choices: list[str] = Field(description='A list of multiple choice answers for the question.')
    answer_key: int = Field(description='The list index (0-based) of the correct answer.')
    difficulty: str = Field(description='easy, medium, or hard.')

    def to_sql_tuple(self):
        """Convert model to a tuple for SQLite insertion."""
        # Note: 'id' is excluded here as it's typically auto-incremented by the DB
        return (
            self.question, json.dumps(self.choices), self.answer_key, self.difficulty
        )

    @classmethod
    def from_sql_row(cls, row):
        """Convert SQLite row to Pydantic model."""
        return cls(
            id=row[0],  # The DB's auto-generated primary key
            question=row[1],
            choices=json.loads(row[2]),
            answer_key=row[3],
            difficulty=row[4]
        )


class QuestionBank(BaseModel):
    """
    Represents a collection of questions associated with a specific document.
    """
    questions: list[QuestionAnswer] = Field(default_factory=list)

    def save_to_db(self, connection: sqlite3.Connection):
        """Save question bank and its questions to SQLite."""
        cursor = connection.cursor()

        for question in self.questions:
            cursor.execute(
                '''
                INSERT INTO questions (question, choices, answer_key, difficulty)
                VALUES (?, ?, ?, ?)
                ''',
                question.to_sql_tuple()
            )
        connection.commit()
        print(f'Saved {len(self.questions)} questions to DB.')

    @classmethod
    def load_from_db(cls, connection: sqlite3.Connection):
        """Load question bank from SQLite."""
        cursor = connection.cursor()
        cursor.execute('SELECT * FROM questions')
        rows = cursor.fetchall()
        if not rows:
            return None

        questions = [QuestionAnswer.from_sql_row(row) for row in rows]
        print(f'Loaded {len(questions)} questions from DB.')
        return cls(questions=questions)

    def filter_by_difficulty(self, difficulty: str) -> list[QuestionAnswer]:
        """Filter questions by difficulty."""
        return [q for q in self.questions if q.difficulty == difficulty]

    def get_random_questions(self, num_questions: int = 5) -> list[QuestionAnswer]:
        """Get a random selection of questions."""
        return random.sample(self.questions, min(num_questions, len(self.questions)))

    def get_random_questions_by_difficulty(
            self,
            difficulty: str,
            num_questions: int = 5
    ) -> list[QuestionAnswer]:
        """Get random questions filtered by difficulty."""
        filtered = self.filter_by_difficulty(difficulty)
        return random.sample(filtered, min(num_questions, len(filtered)))


def extract_as_markdown(
        url_or_file_path: str,
        scrub_links: bool = True,
        max_length: int = 64_000
) -> str:
    """
    Extract the contents from HTML files (.html), PDF files (.pdf), Word Documents (.docx),
    and Excel spreadsheets (.xlsx) as Markdown text. No other file type is supported.
    The text can be used for analysis with LLMs. Input can be a URL or a local file path.
    This tool can directly work with URLs, so no need to download the files separately.
    NOTE: The output returned by this function can be long and may involve lots of quote marks.

    Args:
        url_or_file_path: URL or Path to a .html, .pdf, .docx, or .xlsx file.
        scrub_links: Defaults to `True`, which removes all links from the extracted Markdown text.
          Set it to `False` if you want to retain the links in the text.
        max_length: Limit the output to the first `max_length` characters. Defaults to 64,000.

    Returns:
        The content of the file in Markdown format.
    """
    print('ðŸ›  extract_as_markdown() called')
    md = MarkItDown(enable_plugins=False)

    try:
        result = md.convert(url_or_file_path.strip()).text_content

        if mimetypes.guess_type(url_or_file_path)[0] == 'application/pdf':
            # Handling (cid:NNN) occurrences specific to some PDF extractions
            cid_pattern = re.compile(r'\(cid:(\d+)\)')
            matches = set(cid_pattern.findall(result))
            for cid_num in matches:
                cid_str = f'(cid:{cid_num})'
                result = result.replace(cid_str, chr(int(cid_num) + 29))

        if scrub_links:
            # Remove Markdown links [text](url)
            result = re.sub(r'\[([^\]]+)\]\((https?:\/\/[^\)]+)\)', r'\1', result)

        if max_length is not None:
            result = result[:max_length]

        return result
    except Exception as e:
        # Log the full exception for debugging, return error message to agent
        logging.error('Error extracting markdown from %s: %s', url_or_file_path, str(e))
        return f'Error extracting text: {str(e)}'


async def create_question_bank(text: str) -> QuestionBank:
    """
    Create a question bank (a list of question-answer pairs) based on the provided text.
    Each question will have multiple choices, with one correct answer.

    Args:
        text: The text content from which to generate the question bank.

    Returns:
        A QuestionBank object containing a list of questions.
    """
    print('ðŸ›  create_question_bank() called')

    if not text or len(text.strip()) < 50:  # Basic check for meaningful text
        print('CRITICAL ERROR: Input text to create_question_bank is empty or too short.')
        return QuestionBank(questions=[])

    response = await litellm.acompletion(
        model=MODEL_GEMINI,
        messages=[{'role': 'user', 'content': QUESTION_BANK_PROMPT.format(text=text)}],
        response_format=QuestionBank
    )
    response = response.choices[0].message.content
    qbank: QuestionBank = QuestionBank.model_validate_json(response)
    print(f'Number of questions from LLM Output: {len(qbank.questions)}')

    # Database interaction with raw SQLite3
    connection = get_db_connection()
    try:
        qbank.save_to_db(connection)  # Use the Pydantic model's save_to_db method
    except Exception as db_error:
        print(f'ERROR: Database save failed: {db_error}')
        raise

    return qbank


def create_quiz(
        num_items: int = 5,
        easy: float = 0.4,
        medium: float = 0.3,
) -> list[QuestionAnswer]:
    """
    Create a quiz with a specified number of questions.
    The `easy` and `medium` parameters determine the proportions of easy and medium questions.
    Default values are used unless otherwise specified.
    The proportions of hard questions will be `1 - easy - medium`.
    IMPORTANT: This tool can be called as many times as needed (asked by the user)!

    Args:
        num_items: The number of questions to include in the quiz.
        easy: Proportion of easy questions (default is 0.4).
        medium: Proportion of medium questions (default is 0.3).

    Returns:
        A list of selected questions for the quiz.
    """
    print('ðŸ›  create_quiz() called')

    qbank = QuestionBank.load_from_db(get_db_connection())
    if not qbank or not qbank.questions:
        print('No question bank found or it contains no questions!')
        return []

    easy_questions = qbank.filter_by_difficulty(DIFFICULTY_LEVELS[0])
    medium_questions = qbank.filter_by_difficulty(DIFFICULTY_LEVELS[1])
    hard_questions = qbank.filter_by_difficulty(DIFFICULTY_LEVELS[2])
    print(f'{len(easy_questions)=}, {len(medium_questions)=}, {len(hard_questions)=}')

    # Determine the number of questions per difficulty
    num_easy = int(num_items * easy)
    num_medium = int(num_items * medium)
    num_hard = num_items - num_easy - num_medium
    print(f'{num_easy=}, {num_medium=}, {num_hard=}')

    # Randomly select questions while handling cases where there aren't enough
    easy_questions = random.sample(easy_questions, min(num_easy, len(easy_questions)))
    medium_questions = random.sample(medium_questions, min(num_medium, len(medium_questions)))
    hard_questions = random.sample(hard_questions, min(num_hard, len(hard_questions)))

    # Combine and shuffle the final quiz
    quiz_questions = easy_questions + medium_questions + hard_questions
    random.shuffle(quiz_questions)

    print(f'Generated quiz with {len(quiz_questions)} questions')
    return quiz_questions


def get_db_connection() -> sqlite3.Connection:
    """Return the connection to the in-memory SQLite database."""
    return st.session_state.db_conn


def create_db_tables() -> sqlite3.Connection | None:
    """
    Create a SQLite database and table for questions using raw SQL.
    This function will be called once at the beginning of the script.

    Returns:
        The connection object to the SQLite database or `None` if an error occurs.
    """
    connection = None
    try:
        connection = sqlite3.connect(':memory:', check_same_thread=False)
        cursor = connection.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS questions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                question TEXT NOT NULL,
                choices TEXT NOT NULL, -- Stored as JSON string
                answer_key INTEGER NOT NULL,
                difficulty TEXT NOT NULL
            )
        ''')
        connection.commit()
        print('Database tables created successfully.')
    except sqlite3.Error as e:
        print(f'Error creating database tables: {e}')

    return connection


def evaluate_quiz() -> list[tuple[int, str]]:
    """
    Evaluates all user answers for the current quiz stored in session state.

    Returns:
        A list of tuples: (0 or 1 if the answer is correct,
        and a string with the correct answer if wrong) for each question.
    """
    quiz = st.session_state.get('current_quiz', [])
    answers = st.session_state.get('user_answers', [])
    quiz_results = []

    for idx, question in enumerate(quiz):
        try:
            user_answer = answers[idx]
            user_answer_index = question.choices.index(user_answer)
            is_correct = user_answer_index == question.answer_key
            if is_correct:
                quiz_results.append((1, ''))
            else:
                correct_str = (
                    f'The correct answer is: **{question.choices[question.answer_key]}**.'
                )
                quiz_results.append((0, correct_str))
        except Exception as e:
            quiz_results.append((0, f'Error evaluating answer for question {idx + 1}: {e}'))

    # Save results in session state for later use
    st.session_state.quiz_results = quiz_results

    return quiz_results


def display_quiz(quiz_questions: list[QuestionAnswer]) -> list[QuestionAnswer]:
    """
    This tool acts as a signal to the Streamlit frontend that a quiz should be
    displayed. It returns the list of quiz questions. The actual rendering of
    Streamlit widgets happens in the main Streamlit script, which interprets
    the output of this tool.
    IMPORTANT: This tool can be called as many times as needed (asked by the user)!

    Args:
        quiz_questions: A list of QuestionAnswer objects to be displayed.

    Returns:
        The same list of QuestionAnswer objects, which the Streamlit app will
        then use to render the interactive quiz.
    """
    print('ðŸ›  display_quiz() called')
    if not quiz_questions:
        print('No questions provided to display_quiz tool.')

    return quiz_questions


async def create_agents() -> Agent:
    """
    Create and return the root agent for the application.

    Returns:
        An instance of `Agent` configured for quiz generation.
    """
    ingestion_agent = Agent(
        name='DataIngestionAgent',
        model=llm,
        description=(
            'Ingests data from files or websites (URLs), based on which question-answer pairs'
            ' (question bank) are generated and stored in a database. The database can be used'
            ' by other agents to create quizzes.'
        ),
        instruction=(
            'You are the data ingestion agent supporting quiz generation. Your role is two-fold.'
            ' First, You help with data ingestion by extracting the contents of files using the'
            ' `extract_as_markdown` tool. You can work with local file paths and URLs.'
            ' Second, based on the ingested contents, you also generate a set of question-answer'
            ' pairs (question bank) with different difficulty levels and save them in a database'
            ' using the `create_question_bank` tool. You do not perform any other tasks.'
            ' A user can ask to ingest data or create question banks or quizzes any number'
            ' of times -- you must comply each time. To create or display a quiz, you must'
            ' transfer the query to the `QuizMasterAgent`.'
            ' If you do not know how to answer a question or what tool to use, use the'
            ' `transfer_to_agent` tool to transfer the query to the `CoordinatorAgent`.'
        ),
        tools=[extract_as_markdown, create_question_bank],
    )
    logging.info('Agent `%s` created using model `%s`', ingestion_agent.name, MODEL_GEMINI)

    quizmaster_agent = Agent(
        name='QuizMasterAgent',
        model=llm,
        description=(
            'Generates quizzes based on the question bank (database), displays questions '
            'to the user, and evaluates answers.'
        ),
        instruction=(
            'You are the quiz master agent. Your role is to create quizzes based on the'
            ' question bank (existing database) already created by `DataIngestionAgent`. To'
            ' generate a quiz, you select questions from the question bank based on difficulty'
            ' levels and the number of items requested, if any, using the `create_quiz` tool.'
            ' After successfully creating a quiz, you **must** pass the list of questions '
            ' generated by `create_quiz` to the `display_quiz` tool. This is how you "display" or'
            ' "show" the quiz to the users.'
            ' A user can ask to create quizzes or display them any number of times.'
            ' You MUST comply each time by creating a new quiz or displaying the existing quiz.'
            ' You can use the tools `create_quiz` and `display_quiz` as many times as needed.'
            ' If you do not know how to answer a question or what tool to use, use the'
            ' `transfer_to_agent` tool to transfer the query to the `CoordinatorAgent`.'
        ),
        tools=[create_quiz, display_quiz],
    )
    logging.info(
        'Agent `%s` created using model `%s`', quizmaster_agent.name, MODEL_GEMINI
    )

    coordinator_agent = Agent(
        name='CoordinatorAgent',
        model=llm,
        description=(
            'Coordinates between the data ingestion and quiz master agents.'
        ),
        instruction=(
            'You are the coordinator agent. You play a critical role in identifying which task'
            ' should be delegated to which agent reliably. If a query provides/relates to a data'
            ' source, either a file path or URL, you typically need to delegate to the'
            ' `DataIngestionAgent`. If the user asks to generate a question bank, you should use'
            ' the `extract_as_markdown` tool first to get the content from the provided URL or'
            ' file path, and then pass that content to the `create_question_bank` tool.'
            ' On the other hand, if a query relates to creating or fetching or displaying a quiz,'
            ' you should delegate to the `QuizMasterAgent`. When delegating to QuizMasterAgent'
            ' to create a quiz, ensure it uses the `create_quiz` tool and then the `display_quiz`'
            ' tool. If the user asks for a new quiz or wants to restart, delegate to the'
            ' `QuizMasterAgent` to call `create_quiz` again.'
        ),
        sub_agents=[ingestion_agent, quizmaster_agent],
    )
    logging.info(
        'Root Agent `%s` created using model `%s`', coordinator_agent.name, MODEL_GEMINI
    )

    return coordinator_agent


async def display_agent_response_in_chat(query: str, runner: Runner, user_id: str, session_id: str):
    """
    Send a query to the agent and display the response in Streamlit chat.
    Also, handle special messages based on agent's output.
    """
    st.session_state.messages.append({'role': 'user', 'content': query})
    with st.chat_message('user'):
        st.markdown(query)

    with st.chat_message('assistant'):
        response_placeholder = st.empty()
        full_response_text = ''
        tool_code_displayed_for_turn = False

        async for event in runner.run_async(
                user_id=user_id,
                session_id=session_id,
                new_message=types.Content(role='user', parts=[types.Part(text=query)])
        ):
            # Check for transfer to another agent
            if event.actions and getattr(
                    event.actions, 'transfer_to_agent', None
            ) and not tool_code_displayed_for_turn:
                transfer_info = event.actions.transfer_to_agent
                st.info(f'Transferring to agent: {transfer_info}')
                tool_code_displayed_for_turn = True
                response_placeholder.empty()
                full_response_text = ''

            # Check for function call in event content
            if event.content and event.content.parts:
                part = event.content.parts[0]
                if getattr(part, 'function_call', None) and not tool_code_displayed_for_turn:
                    func_call = part.function_call
                    st.info(
                        f'Calling function: `{func_call.name}` with arguments:\n'
                        f'```json\n{json.dumps(func_call.args, indent=2)}\n```'
                    )
                    tool_code_displayed_for_turn = True
                    response_placeholder.empty()
                    full_response_text = ''

                if getattr(part, 'function_response', None):
                    func_response = part.function_response.response
                    func_name = part.function_response.name

                    if func_name == 'display_quiz':
                        try:
                            quiz_questions = [
                                QuestionAnswer.model_validate(q) for q in func_response['result']
                            ]
                            if quiz_questions:
                                print('-> Saved quiz in session')
                                save_and_display_quiz_session(quiz_questions)
                                # Rerun streamlit to display the quiz
                                st.rerun()
                            else:
                                text = 'The agent attempted to display an empty quiz.'
                                st.warning(text)
                                full_response_text += text
                        except Exception as e:
                            st.error(f'Error parsing quiz questions from display_quiz tool: {e}')
                            full_response_text += f'Error parsing quiz questions: {e}'

            if event.content and event.content.parts:
                part_text = event.content.parts[0].text
                if part_text:
                    full_response_text += part_text
                response_placeholder.markdown(full_response_text + 'â–Œ')

            if event.is_final_response():
                print('Received final response...')
                print(f'{event=}')
                if event.error_message:
                    print(f'Agent error: {event.error_message}')
                    response_placeholder.error(f'Agent error: {event.error_message}')
                    st.session_state.messages.append(
                        {'role': 'assistant', 'content': f'Agent error: {event.error_message}'})

                if full_response_text:
                    st.session_state.messages.append(
                        {'role': 'assistant', 'content': full_response_text})

                response_placeholder.empty()
                if full_response_text:
                    st.markdown('âœ¨ ' + full_response_text)
                break


def save_and_display_quiz_session(quiz_questions: Optional[list[QuestionAnswer]] = None):
    """
    Save the quiz questions in the session state of Streamlit and display a message in the chat.

    Args:
        quiz_questions: A list of QuestionAnswer objects representing the quiz questions.
    """
    # Save quiz state in session
    if quiz_questions:
        st.session_state.current_quiz = quiz_questions
        st.session_state.quiz_submitted = False
        st.session_state.messages.append({
            'role': 'assistant',
            'content': f'Here is your quiz with {len(quiz_questions)} questions.'
        })


if 'db_conn' not in st.session_state:
    st.session_state.db_conn = create_db_tables()

if 'agent_runner' not in st.session_state:
    st.session_state.session_service = InMemorySessionService()
    st.session_state.user_id = 'streamlit_user'
    st.session_state.session_id = str(uuid.uuid4())
    st.session_state.agent_runner = None
    st.session_state.question_bank = None
    st.session_state.current_quiz = None
    st.session_state.quiz_results = []
    st.session_state.messages = []


@st.cache_resource
def get_agent_runner(session_id: str):
    """
    Initialize the agent runner with the root agent and session service.
    This function is cached to avoid re-initialization on every app rerun.

    Args:
        session_id: A unique identifier for the session, used to manage state.

    Returns:
        Runner: An instance of the Runner class with the root agent and session service.
    """
    session_service = InMemorySessionService()
    user_id = 'streamlit_user'
    # Use asyncio.run for the initial async setup in a synchronous Streamlit context
    asyncio.run(session_service.create_session(
        app_name=APP_NAME,
        user_id=user_id,
        session_id=session_id
    ))
    agent = asyncio.run(create_agents())
    return Runner(
        agent=agent,
        app_name=APP_NAME,
        session_service=session_service
    )


st.session_state.agent_runner = get_agent_runner(st.session_state.session_id)

st.markdown(
    """
    You can chat with QuizAlchemy to:
    - **Generate a question bank**: Provide a URL
      (e.g., `Generate question bank from https://example.com/doc.pdf`).
    - **Create a quiz**: Ask `Create a quiz with 5 questions`. 
      The quiz will then appear below the chat.
    - **Transfer/escalate**: Sometimes the agents might fail to do what asked. You can type
      `coordinator` to transfer the query to the `CoordinatorAgent`.

    Example: `Create a quiz from 
    https://web.stanford.edu/class/cs102/lectureslides/ClassificationSlides.pdf` and display it
    """
)

# Display chat messages from history on app rerun
for message in st.session_state.messages:
    with st.chat_message(message['role']):
        st.markdown(message['content'])

# Display quiz questions if a quiz is active
_active_quiz = st.session_state.current_quiz
_quiz_results = st.session_state.get('quiz_results', [])

# Placeholders to display feedback for each question later
feedback_placeholders = []

if _active_quiz and not st.session_state.quiz_submitted:
    st.divider()
    user_answers = []
    for _idx, _question in enumerate(_active_quiz):
        st.subheader(f'Question {_idx + 1}/{len(_active_quiz)}')
        st.markdown(f'**{_question.question}**')
        selected = st.radio(
            'Select your answer:',
            options=_question.choices,
            key=f'quiz_q{_idx}',
        )
        # Create a placeholder for feedback
        feedback_placeholder = st.empty()
        feedback_placeholders.append(feedback_placeholder)
        user_answers.append(selected)

    st.session_state.user_answers = user_answers
    st.divider()

    col1, col2 = st.columns([1, 1])
    with col1:
        if st.button('Submit Answer'):
            results = evaluate_quiz()
            # Display feedback for each question after evaluation
            for _idx, (_is_correct, feedback) in enumerate(st.session_state.quiz_results):
                if _is_correct:
                    feedback_placeholders[_idx].success('That is correct! ðŸŽ‰')
                else:
                    feedback_placeholders[_idx].error(feedback)
            st.session_state.quiz_submitted = True
    with col2:
        if st.button('New Quiz'):
            # Clear existing quiz state and ask the agent to create a new one
            st.session_state.current_quiz = None
            st.session_state.quiz_submitted = False
            # Send a direct query to the agent to create a new quiz
            asyncio.run(display_agent_response_in_chat(
                'Create a new quiz with 5 questions & display the quiz.',
                st.session_state.agent_runner,
                st.session_state.user_id,
                st.session_state.session_id
            ))


# Accept user input via chat_input
if prompt := st.chat_input(
        'Type your command here (e.g., "Create & show a quiz from URL")'
):
    print(f'{prompt=}')
    with st.spinner('Processing your request...'):
        asyncio.run(display_agent_response_in_chat(
            prompt,
            st.session_state.agent_runner,
            st.session_state.user_id,
            st.session_state.session_id
        ))
